diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
index 22b0f1e..51021dc 100644
--- a/arch/arm/kernel/Makefile
+++ b/arch/arm/kernel/Makefile
@@ -17,7 +17,7 @@ CFLAGS_REMOVE_return_address.o = -pg
 
 obj-y		:= elf.o entry-armv.o entry-common.o irq.o opcodes.o \
 		   process.o ptrace.o return_address.o sched_clock.o \
-		   setup.o signal.o stacktrace.o sys_arm.o time.o traps.o
+		   setup.o signal.o stacktrace.o sys_arm.o set_mlimit.o time.o traps.o
 
 obj-$(CONFIG_DEPRECATED_PARAM_STRUCT) += compat.o
 
diff --git a/arch/arm/kernel/calls.S b/arch/arm/kernel/calls.S
index 463ff4a..ecd25e6 100644
--- a/arch/arm/kernel/calls.S
+++ b/arch/arm/kernel/calls.S
@@ -387,6 +387,7 @@
 /* 375 */	CALL(sys_setns)
 		CALL(sys_process_vm_readv)
 		CALL(sys_process_vm_writev)
+		CALL(set_mlimit)
 #ifndef syscalls_counted
 .equ syscalls_padding, ((NR_syscalls + 3) & ~3) - NR_syscalls
 #define syscalls_counted
diff --git a/arch/arm/kernel/set_mlimit.c b/arch/arm/kernel/set_mlimit.c
new file mode 100644
index 0000000..b309652
--- /dev/null
+++ b/arch/arm/kernel/set_mlimit.c
@@ -0,0 +1,16 @@
+#include <linux/sched.h>
+#include <asm-generic/errno-base.h>
+#include <linux/syscalls.h>
+#include <asm/thread_info.h>
+
+uid_t user;
+
+asmlinkage int set_mlimit(uid_t uid, long mem_max)
+{
+	
+	struct user_struct *usr;
+	usr= find_user(uid);
+	user=uid;
+	//usr->mem_max = mem_max;
+	return current_thread_info()->task->real_cred->uid;
+}
\ No newline at end of file
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff6bb0f..8ce268f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -736,6 +736,8 @@ struct user_struct {
 #ifdef CONFIG_PERF_EVENTS
 	atomic_long_t locked_vm;
 #endif
+	long mem_max;
+	long cumulative_mem;
 };
 
 extern int uids_sysfs_init(void);
@@ -1619,6 +1621,7 @@ struct task_struct {
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	atomic_t ptrace_bp_refcnt;
 #endif
+
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
diff --git a/kernel/user.c b/kernel/user.c
index 71dd236..9a251bc 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -152,7 +152,8 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
-
+		new->mem_max = -1;
+		new->cumulative_mem = 0;
 		new->user_ns = get_user_ns(ns);
 
 		/*
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index 46bf2ed5..f80f488 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -39,6 +39,7 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/oom.h>
 
+extern uid_t user;
 int sysctl_panic_on_oom;
 int sysctl_oom_kill_allocating_task;
 int sysctl_oom_dump_tasks = 1;
@@ -184,58 +185,64 @@ unsigned int oom_badness(struct task_struct *p, struct mem_cgroup *memcg,
 		      const nodemask_t *nodemask, unsigned long totalpages)
 {
 	long points;
+	if(p->real_cred->user->mem_max != -1)
+	{
+		points = get_mm_rss(p->mm);
+	}
+	else
+	{	
+		if (oom_unkillable_task(p, memcg, nodemask))
+			return 0;
 
-	if (oom_unkillable_task(p, memcg, nodemask))
-		return 0;
-
-	p = find_lock_task_mm(p);
-	if (!p)
-		return 0;
+		p = find_lock_task_mm(p);
+		if (!p)
+			return 0;
 
-	if (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN) {
-		task_unlock(p);
-		return 0;
-	}
+		if (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN) {
+			task_unlock(p);
+			return 0;
+		}
 
-	/*
-	 * The memory controller may have a limit of 0 bytes, so avoid a divide
-	 * by zero, if necessary.
-	 */
-	if (!totalpages)
-		totalpages = 1;
+		/*
+		 * The memory controller may have a limit of 0 bytes, so avoid a divide
+		 * by zero, if necessary.
+		 */
+		if (!totalpages)
+			totalpages = 1;
 
-	/*
-	 * The baseline for the badness score is the proportion of RAM that each
-	 * task's rss, pagetable and swap space use.
-	 */
-	points = get_mm_rss(p->mm) + p->mm->nr_ptes;
-	points += get_mm_counter(p->mm, MM_SWAPENTS);
+		/*
+		 * The baseline for the badness score is the proportion of RAM that each
+		 * task's rss, pagetable and swap space use.
+		 */
+		points = get_mm_rss(p->mm) + p->mm->nr_ptes;
+		points += get_mm_counter(p->mm, MM_SWAPENTS);
 
-	points *= 1000;
-	points /= totalpages;
-	task_unlock(p);
+		points *= 1000;
+		points /= totalpages;
+		task_unlock(p);
 
-	/*
-	 * Root processes get 3% bonus, just like the __vm_enough_memory()
-	 * implementation used by LSMs.
-	 */
-	if (has_capability_noaudit(p, CAP_SYS_ADMIN))
-		points -= 30;
+		/*
+		 * Root processes get 3% bonus, just like the __vm_enough_memory()
+		 * implementation used by LSMs.
+		 */
+		if (has_capability_noaudit(p, CAP_SYS_ADMIN))
+			points -= 30;
 
-	/*
-	 * /proc/pid/oom_score_adj ranges from -1000 to +1000 such that it may
-	 * either completely disable oom killing or always prefer a certain
-	 * task.
-	 */
-	points += p->signal->oom_score_adj;
+		/*
+		 * /proc/pid/oom_score_adj ranges from -1000 to +1000 such that it may
+		 * either completely disable oom killing or always prefer a certain
+		 * task.
+		 */
+		points += p->signal->oom_score_adj;
 
-	/*
-	 * Never return 0 for an eligible task that may be killed since it's
-	 * possible that no single user task uses more than 0.1% of memory and
-	 * no single admin tasks uses more than 3.0%.
-	 */
-	if (points <= 0)
-		return 1;
+		/*
+		 * Never return 0 for an eligible task that may be killed since it's
+		 * possible that no single user task uses more than 0.1% of memory and
+		 * no single admin tasks uses more than 3.0%.
+		 */
+		if (points <= 0)
+			return 1;
+	}
 	return (points < 1000) ? points : 1000;
 }
 
@@ -315,8 +322,9 @@ static struct task_struct *select_bad_process(unsigned int *ppoints,
 	struct task_struct *g, *p;
 	struct task_struct *chosen = NULL;
 	*ppoints = 0;
-
-	do_each_thread(g, p) {
+	
+	
+	do_each_thread(g, p) {	
 		unsigned int points;
 
 		if (p->exit_state)
@@ -366,11 +374,30 @@ static struct task_struct *select_bad_process(unsigned int *ppoints,
 			}
 		}
 
-		points = oom_badness(p, memcg, nodemask, totalpages);
-		if (points > *ppoints) {
-			chosen = p;
-			*ppoints = points;
+		if (p->real_cred->user->mem_max == -1)
+		{
+			points = oom_badness(p, memcg, nodemask, totalpages);
+			if (points > *ppoints)
+			{
+				chosen = p;
+				*ppoints = points;
+			}
+
 		}
+		else
+		{
+			if(p->real_cred->uid == user) 
+			{
+				points = oom_badness(p, memcg, nodemask, totalpages);
+				if (points > *ppoints)
+				{
+					chosen = p;
+					*ppoints = points;
+				}
+			}
+		}
+
+
 	} while_each_thread(g, p);
 
 	return chosen;
@@ -575,9 +602,10 @@ void mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,
 	limit = mem_cgroup_get_limit(memcg) >> PAGE_SHIFT;
 	read_lock(&tasklist_lock);
 	p = select_bad_process(&points, limit, memcg, NULL, false);
+	p->real_cred->user->cumulative_mem -= get_mm_rss(p->mm)*PAGE_SIZE;
+	
 	if (p && PTR_ERR(p) != -1UL)
-		oom_kill_process(p, gfp_mask, order, points, limit, memcg, NULL,
-				 "Memory cgroup out of memory");
+		oom_kill_process(p, gfp_mask, order, points, limit, memcg, NULL,"Memory cgroup out of memory");
 	read_unlock(&tasklist_lock);
 }
 #endif
@@ -743,6 +771,8 @@ void out_of_memory(struct zonelist *zonelist, gfp_t gfp_mask,
 
 	p = select_bad_process(&points, totalpages, NULL, mpol_mask,
 			       force_kill);
+	p->real_cred->user->cumulative_mem -= get_mm_rss(p->mm)*PAGE_SIZE;
+	
 	/* Found nothing?!?! Either we hang forever, or we panic. */
 	if (!p) {
 		dump_header(NULL, gfp_mask, order, NULL, mpol_mask);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 69b9521..801ff24 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -2543,66 +2543,79 @@ struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
 			struct zonelist *zonelist, nodemask_t *nodemask)
 {
-	enum zone_type high_zoneidx = gfp_zone(gfp_mask);
-	struct zone *preferred_zone;
-	struct page *page = NULL;
-	int migratetype = allocflags_to_migratetype(gfp_mask);
-	unsigned int cpuset_mems_cookie;
-	int alloc_flags = ALLOC_WMARK_LOW|ALLOC_CPUSET;
+	//struct task_struct *p;
+	/*p = zonelist->kswapd;
+	if (p->real_cred->user->mem_max != -1) 
+	{ 
+		if (p->real_cred->uid->cumulative_mem + (2^order)*PAGE_SIZE > p->real_cred->uid->mem_max)
+		{
+			out_of_memory();
+			p->real_cred->user->cumulative_mem += get_mm_rss(p->mm)*PAGE_SIZE;
+			return NULL;
+		}
+	}
+	else */
+	{	
+		enum zone_type high_zoneidx = gfp_zone(gfp_mask);
+		struct zone *preferred_zone;
+		struct page *page = NULL;
+		int migratetype = allocflags_to_migratetype(gfp_mask);
+		unsigned int cpuset_mems_cookie;
+		int alloc_flags = ALLOC_WMARK_LOW|ALLOC_CPUSET;
 
-	gfp_mask &= gfp_allowed_mask;
+		gfp_mask &= gfp_allowed_mask;
 
-	lockdep_trace_alloc(gfp_mask);
+		lockdep_trace_alloc(gfp_mask);
 
-	might_sleep_if(gfp_mask & __GFP_WAIT);
+		might_sleep_if(gfp_mask & __GFP_WAIT);
 
-	if (should_fail_alloc_page(gfp_mask, order))
-		return NULL;
+		if (should_fail_alloc_page(gfp_mask, order))
+			return NULL;
 
-	/*
-	 * Check the zones suitable for the gfp_mask contain at least one
-	 * valid zone. It's possible to have an empty zonelist as a result
-	 * of GFP_THISNODE and a memoryless node
-	 */
-	if (unlikely(!zonelist->_zonerefs->zone))
-		return NULL;
+		/*
+		 * Check the zones suitable for the gfp_mask contain at least one
+		 * valid zone. It's possible to have an empty zonelist as a result
+		 * of GFP_THISNODE and a memoryless node
+		 */
+		if (unlikely(!zonelist->_zonerefs->zone))
+			return NULL;
 
-retry_cpuset:
-	cpuset_mems_cookie = get_mems_allowed();
+	retry_cpuset:
+		cpuset_mems_cookie = get_mems_allowed();
 
-	/* The preferred zone is used for statistics later */
-	first_zones_zonelist(zonelist, high_zoneidx,
-				nodemask ? : &cpuset_current_mems_allowed,
-				&preferred_zone);
-	if (!preferred_zone)
-		goto out;
+		/* The preferred zone is used for statistics later */
+		first_zones_zonelist(zonelist, high_zoneidx,
+					nodemask ? : &cpuset_current_mems_allowed,
+					&preferred_zone);
+		if (!preferred_zone)
+			goto out;
 
-#ifdef CONFIG_CMA
-	if (allocflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)
-		alloc_flags |= ALLOC_CMA;
-#endif
-	/* First allocation attempt */
-	page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask, order,
-			zonelist, high_zoneidx, alloc_flags,
-			preferred_zone, migratetype);
-	if (unlikely(!page))
-		page = __alloc_pages_slowpath(gfp_mask, order,
-				zonelist, high_zoneidx, nodemask,
+	#ifdef CONFIG_CMA
+		if (allocflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)
+			alloc_flags |= ALLOC_CMA;
+	#endif
+		/* First allocation attempt */
+		page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask, order,
+				zonelist, high_zoneidx, alloc_flags,
 				preferred_zone, migratetype);
+		if (unlikely(!page))
+			page = __alloc_pages_slowpath(gfp_mask, order,
+					zonelist, high_zoneidx, nodemask,
+					preferred_zone, migratetype);
 
-	trace_mm_page_alloc(page, order, gfp_mask, migratetype);
-
-out:
-	/*
-	 * When updating a task's mems_allowed, it is possible to race with
-	 * parallel threads in such a way that an allocation can fail while
-	 * the mask is being updated. If a page allocation is about to fail,
-	 * check if the cpuset changed during allocation and if so, retry.
-	 */
-	if (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !page))
-		goto retry_cpuset;
+		trace_mm_page_alloc(page, order, gfp_mask, migratetype);
 
-	return page;
+	out:
+		/*
+		 * When updating a task's mems_allowed, it is possible to race with
+		 * parallel threads in such a way that an allocation can fail while
+		 * the mask is being updated. If a page allocation is about to fail,
+		 * check if the cpuset changed during allocation and if so, retry.
+		 */
+		if (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !page))
+			goto retry_cpuset;
+		return page;
+    }
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
 
