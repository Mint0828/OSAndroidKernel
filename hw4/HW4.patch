diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff6bb0f..bfb4386 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -39,6 +39,7 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+#define SCHED_MYCFS		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1232,6 +1233,7 @@ struct sched_entity {
 #endif
 };
 
+
 struct sched_rt_entity {
 	struct list_head run_list;
 	unsigned long timeout;
@@ -1248,6 +1250,13 @@ struct sched_rt_entity {
 #endif
 };
 
+// PJ: populate it with entries similar to struct_entity
+struct sched_mycfs_entity{
+        struct sched_mycfs_entity	*parent;
+        unsigned int time_slice;
+	u64			exec_start;
+	u64			vruntime;
+};
 /*
  * default timeslice is 100 msecs (used only for SCHED_RR tasks).
  * Timeslices get refilled after they expire.
@@ -1619,6 +1628,7 @@ struct task_struct {
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	atomic_t ptrace_bp_refcnt;
 #endif
+	struct sched_mycfs_entity mycfs_se;
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 3ede7d9..39c672d 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o sched_avg.o
+obj-y += core.o clock.o idle_task.o mycfs.o fair.o rt.o stop_task.o sched_avg.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1cee48f..ce0841e 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4063,6 +4063,16 @@ __setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
 		p->sched_class = &rt_sched_class;
 	else
 		p->sched_class = &fair_sched_class;
+
+	if(policy == SCHED_MYCFS)
+	{
+		//PJ 
+		//p->sched_class = &fair_sched_class;
+		p->sched_class = &mycfs_sched_class;
+		//p->sched_class = &stop_sched_class;
+		//p->sched_class = &rt_sched_class;
+		printk("process %d is assigned mycfs scheduler\n",p->pid);
+	}
 	set_load_weight(p);
 }
 
@@ -4107,7 +4117,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_IDLE && policy!= SCHED_MYCFS)
 			return -EINVAL;
 	}
 
@@ -4783,10 +4793,13 @@ SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
 		ret = MAX_USER_RT_PRIO-1;
 		break;
 	case SCHED_NORMAL:
+	case SCHED_MYCFS:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
 		ret = 0;
 		break;
+
+
 	}
 	return ret;
 }
@@ -4804,6 +4817,7 @@ SYSCALL_DEFINE1(sched_get_priority_min, int, policy)
 
 	switch (policy) {
 	case SCHED_FIFO:
+	case SCHED_MYCFS:
 	case SCHED_RR:
 		ret = 1;
 		break;
@@ -7003,6 +7017,7 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+		init_mycfs_rq(&rq->mycfs);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index fc60d5b..d240ab2 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5536,7 +5536,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &mycfs_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/kernel/sched/mycfs.c b/kernel/sched/mycfs.c
new file mode 100644
index 0000000..ccaa6fd
--- /dev/null
+++ b/kernel/sched/mycfs.c
@@ -0,0 +1,104 @@
+#include <linux/sched.h>
+//#include "mycfs.h"
+#include "sched.h"
+
+static void enqueue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	p->on_rq = 1;
+	//inc_nr_running(rq);
+}
+
+static void dequeue_task_mycfs(struct rq *rq, struct task_struct *prev,int flags)
+{
+	prev->on_rq = 0;
+	//dec_nr_running(rq);
+}
+
+//this will be the big one to implement!!!
+static struct task_struct *pick_next_task_mycfs(struct rq *rq)
+{
+	
+	struct task_struct *curr = rq->curr;
+	
+	if (curr && curr->on_rq)
+	{
+		printk("pick_next_task_mycfs: Process %d eligible to run!\n", curr->pid);	
+		return curr;
+	}
+	//printk("pick_next_task_mycfs: no processes eligible to be run\n");
+	return NULL;
+}
+
+static void put_prev_task_mycfs(struct rq *rq, struct task_struct *prev)
+{
+	//printk("put_prev_task_mycfs\n");
+}
+
+
+
+static void yield_task_mycfs(struct rq *rq)
+{
+}
+
+static void check_preempt_curr_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	//resched_task(rq->curr);
+}
+
+static int select_task_rq_mycfs(struct task_struct *p, int sd_flag, int wake_flags)
+{
+
+	return task_cpu(p); 
+	
+}
+
+static void set_curr_task_mycfs(struct rq *rq)
+{
+}
+
+static unsigned int get_rr_interval_mycfs(struct rq *rq, struct task_struct *task)
+{
+	return 0;
+}
+
+static void
+prio_changed_mycfs(struct rq *rq, struct task_struct *p, int oldprio)
+{
+}
+
+static void task_tick_mycfs(struct rq *rq, struct task_struct *curr, int queued)
+{
+}
+
+static void task_fork_mycfs(struct task_struct *p)
+{
+}
+
+void init_mycfs_rq(struct mycfs_rq *mycfs_rq)
+{
+	//creating the RB tree for MYCFS:
+	mycfs_rq->tasks_timeline = RB_ROOT;
+	mycfs_rq->min_vruntime = (u64)(-(1LL << 20));
+#ifndef CONFIG_64BIT
+	mycfs_rq->min_vruntime_copy = mycfs_rq->min_vruntime;
+#endif
+}
+
+const struct sched_class mycfs_sched_class = {
+	.next = &idle_sched_class,
+	.dequeue_task	= dequeue_task_mycfs,
+	.pick_next_task = pick_next_task_mycfs,
+	.enqueue_task = enqueue_task_mycfs,
+	.put_prev_task	= put_prev_task_mycfs,
+	.yield_task		= yield_task_mycfs,
+	.check_preempt_curr	= check_preempt_curr_mycfs,
+	#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_mycfs,
+	#endif
+	.set_curr_task		= set_curr_task_mycfs,
+	.get_rr_interval	= get_rr_interval_mycfs,
+	.prio_changed		= prio_changed_mycfs,
+	.task_tick		= task_tick_mycfs,
+	.task_fork		= task_fork_mycfs,
+};
+
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5370bcb..f38e101 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -273,6 +273,19 @@ struct cfs_rq {
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
 
+//PJ: to be populated with many fields
+struct mycfs_rq 
+{
+	unsigned long nr_running, h_nr_running;
+	u64 min_vruntime;
+#ifndef CONFIG_64BIT
+	u64 min_vruntime_copy;
+#endif
+	struct rb_root tasks_timeline;
+	struct rb_node *rb_leftmost;
+	struct sched_entity *curr, *next, *last, *skip;
+};
+
 static inline int rt_bandwidth_enabled(void)
 {
 	return sysctl_sched_rt_runtime >= 0;
@@ -465,6 +478,7 @@ struct rq {
 #ifdef CONFIG_SMP
 	struct llist_head wake_list;
 #endif
+	struct mycfs_rq mycfs;
 };
 
 static inline int cpu_of(struct rq *rq)
@@ -856,6 +870,7 @@ enum cpuacct_stat_index {
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class mycfs_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
@@ -1155,6 +1170,9 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+// TG
+extern void init_mycfs_rq(struct mycfs_rq *mycfs_rq);
+
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
